# ------------------------------------------------------------
# WhereIsThisPlace – backend image (TorchServe + FastAPI)
# ------------------------------------------------------------
# CUDA 12 / T4-compatible base with TorchServe pre-installed
FROM pytorch/torchserve:0.9.2-gpu

# 1. Copy application source
WORKDIR /app
COPY api/      ./api
COPY ml/       ./ml
COPY models/   ./models          # optional – .mar files mounted later

# 2. Install Python runtime deps (exported from Poetry)
#    – Torch & CUDA already come with the base image
RUN pip install --no-cache-dir poetry==1.8.3 && \
    poetry --directory ./api export --without-hashes --only main -f requirements.txt | \
    pip install --no-cache-dir -r /dev/stdin && \
    pip install --no-cache-dir uvicorn[standard] faiss-gpu==1.7.4

# 3. Optional: prepare model-store (if a .mar is present)
RUN mkdir -p /model-store && \
    if [ -f models/where-v1.mar ]; then \
        cp models/where-v1.mar /model-store/; \
    fi

# 4. Expose ports
#    8080 – TorchServe inference API
#    8081 – FastAPI
EXPOSE 8080 8081

# 5. Entrypoint: start TorchServe, wait 5 s, then FastAPI
COPY api/docker/start.sh /start.sh
RUN chmod +x /start.sh
CMD ["/start.sh"]
