# WhereIsThisPlace GPU backend image
FROM pytorch/torchserve:0.10.0-gpu

# Add build arg to bust cache
ARG CACHEBUST=1

# Install Poetry and project dependencies
USER root
WORKDIR /app

# Copy Poetry files first (for better Docker layer caching)
COPY pyproject.toml poetry.lock ./

# Install Poetry
RUN echo "Cache bust: $CACHEBUST" && \
    pip install --no-cache-dir poetry

# Configure Poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    poetry install --only=main --no-dev

# Copy application code
COPY api/ /app/api/

# Create model store and copy any models that exist
RUN mkdir -p /model-store
COPY models/ /model-store/

# Copy ML code
RUN mkdir -p /app/ml
COPY ml/ /app/ml/

# Provide fallback TorchServe configuration
RUN mkdir -p /app/config && \
    echo "inference_address=http://0.0.0.0:8080" > /app/config/config.properties && \
    echo "management_address=http://0.0.0.0:8081" >> /app/config/config.properties && \
    echo "metrics_address=http://0.0.0.0:8082" >> /app/config/config.properties && \
    echo "model_store=/model-store" >> /app/config/config.properties && \
    echo "load_models=all" >> /app/config/config.properties

USER model-server

# Start both TorchServe and FastAPI
CMD ["bash", "-c", "torchserve --start --ncs --model-store /model-store --models all --ts-config /app/config/config.properties & sleep 30 && cd /app && python -m uvicorn api.main:app --host 0.0.0.0 --port 8000"]