# ------------------------------------------------------------
# WhereIsThisPlace â€“ GPU backend image (TorchServe + FastAPI)
# ------------------------------------------------------------
FROM pytorch/torchserve:0.10.0-gpu

WORKDIR /app

# 1. Copy source
COPY api/ ./api
COPY ml/  ./ml
# (models/ copy skipped until we actually have .mar)

# 2. Install runtime deps
RUN pip install --no-cache-dir poetry==1.8.3 && \
    poetry --directory ./api export --without-hashes --only main -f requirements.txt | \
    pip install --no-cache-dir -r /dev/stdin && \
    pip install --no-cache-dir uvicorn[standard] faiss-gpu==1.7.4

# 3. TorchServe model-store (optional .mar present)
RUN mkdir -p /model-store && \
    if [ -f models/where-v1.mar ]; then cp models/where-v1.mar /model-store/; fi

EXPOSE 8080 8081

COPY api/docker/start.sh /start.sh
RUN chmod +x /start.sh
CMD ["/start.sh"]
