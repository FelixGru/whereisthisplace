# WhereIsThisPlace GPU backend image
FROM pytorch/torchserve:0.10.0-gpu

# Add build arg to bust cache
ARG CACHEBUST=1

# Set Python path for the application
ENV PYTHONPATH /app

USER root
WORKDIR /app

# Copy Poetry files to the API package root within /app
COPY api/pyproject.toml /app/api/pyproject.toml
COPY api/poetry.lock /app/api/poetry.lock

# Install Poetry and dependencies to the venv that TorchServe uses
RUN echo "Cache bust: $CACHEBUST" && \
    apt-get update && apt-get install -y postgresql-client && \
    /home/venv/bin/pip install --no-cache-dir poetry && \
    /home/venv/bin/poetry config virtualenvs.create false && \
    /home/venv/bin/poetry --directory /app/api install --only main --no-interaction --no-root && \
    /home/venv/bin/pip install --no-cache-dir uvicorn[standard] fastapi && \
    /home/venv/bin/pip install --no-cache-dir asyncpg psycopg2-binary sqlalchemy geoalchemy2 pgvector[sqlalchemy] && \
    /home/venv/bin/pip install --no-cache-dir numpy python-dotenv pydantic-settings requests python-multipart alembic httpx && \
    /home/venv/bin/pip install --no-cache-dir "openai>=1.0.0" && \    
    echo "Installed packages:" && \
    /home/venv/bin/pip list | grep -E "(uvicorn|fastapi|asyncpg|psycopg2|sqlalchemy)" && \
    echo "Python path check:" && \
    /home/venv/bin/python -c "import uvicorn; print('uvicorn imported successfully')" && \
    /home/venv/bin/python -c "import fastapi; print('fastapi imported successfully')" && \
    /home/venv/bin/python -c "import asyncpg; print('asyncpg imported successfully')"

# Copy application code from host's api/ directory to container's /app/api/ directory
# This means your actual FastAPI app (e.g., main.py) should be in repo_root/api/main.py
# or repo_root/api/api/main.py.
# If main.py is in repo_root/api/api/main.py, then the Uvicorn command would be api.api.main:app
# If main.py is in repo_root/api/main.py, then the Uvicorn command is api.main:app (as you have it)
COPY api/ /app/api/

# Create model store and copy any models that exist from host's models/ directory
RUN mkdir -p /model-store
COPY models/ /model-store/

# Copy ML code from host's ml/ directory
RUN mkdir -p /app/ml
COPY ml/ /app/ml/

# Provide fallback TorchServe configuration
RUN mkdir -p /app/config && \
    echo "inference_address=http://0.0.0.0:8080" > /app/config/config.properties && \
    echo "management_address=http://0.0.0.0:8081" >> /app/config/config.properties && \
    echo "metrics_address=http://0.0.0.0:8082" >> /app/config/config.properties && \
    echo "model_store=/model-store" >> /app/config/config.properties && \
    echo "load_models=all" >> /app/config/config.properties

# Create logs directory for TorchServe
RUN mkdir -p /app/logs && \
    chown -R model-server:model-server /app/logs

# Create startup script for better debugging
RUN echo '#!/bin/bash' > /app/start.sh && \
    echo 'set -e' >> /app/start.sh && \
    echo 'echo "=== Starting services ==="' >> /app/start.sh && \
    echo 'echo "Python version: $(/home/venv/bin/python --version)"' >> /app/start.sh && \
    echo 'echo "Checking uvicorn installation..."' >> /app/start.sh && \
    echo '/home/venv/bin/python -c "import uvicorn; print(f\"uvicorn version: {uvicorn.__version__}\")"' >> /app/start.sh && \
    echo 'echo "Starting TorchServe..."' >> /app/start.sh && \
    echo 'torchserve --start --ncs --model-store /model-store --models all --ts-config /app/config/config.properties &' >> /app/start.sh && \
    echo 'echo "Waiting 30 seconds for TorchServe to start..."' >> /app/start.sh && \
    echo 'sleep 30' >> /app/start.sh && \
    echo 'echo "Starting FastAPI..."' >> /app/start.sh && \
    echo 'cd /app' >> /app/start.sh && \
    echo '/home/venv/bin/python -m uvicorn api.main:app --host 0.0.0.0 --port 8000' >> /app/start.sh && \
    chmod +x /app/start.sh && \
    chown model-server:model-server /app/start.sh

USER model-server

# Start both TorchServe and FastAPI using the startup script
CMD ["/app/start.sh"]
