# ------------------------------------------------------------
# WhereIsThisPlace – GPU backend image (TorchServe + FastAPI + FAISS-GPU)
# With auto-loading models from the image
# ------------------------------------------------------------
FROM pytorch/torchserve:0.10.0-gpu

# Build-time variables & environment
ENV DEBIAN_FRONTEND=noninteractive \
    PATH="/usr/local/cuda/bin:${PATH}" \
    LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    PYTHONPATH="/app:${PYTHONPATH}"

# Explicitly switch to root user for system package installation
USER root

# 1. System dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential git \
    ca-certificates wget && \
    rm -rf /var/lib/apt/lists/*

# 2. Copy application source
WORKDIR /app
COPY api/ ./api
COPY ml/ ./ml

# 3. Fix the import issue in the source code
RUN if [ -f /app/api/api/main.py ]; then \
    sed -i 's/from api.routes.predict import router/from routes.predict import router/' /app/api/api/main.py; \
    echo "Fixed import in api/api/main.py"; \
    elif [ -f /app/api/main.py ]; then \
    sed -i 's/from api.routes.predict import router/from routes.predict import router/' /app/api/main.py; \
    echo "Fixed import in api/main.py"; \
    fi

# 4. Install Python dependencies via Poetry (using the working approach)
RUN pip install --no-cache-dir poetry==1.8.3 && \
    # Export poetry dependencies to requirements.txt and install them
    poetry --directory ./api export --without-hashes --only main -f requirements.txt | \
    pip install --no-cache-dir -r /dev/stdin && \
    # Install additional required packages including FastAPI explicitly
    pip install --no-cache-dir fastapi==0.111.0 'pydantic>=2.7' uvicorn[standard] && \
    # Install database drivers as a safety net in case they're missing from Poetry
    pip install --no-cache-dir asyncpg==0.29.0 psycopg[binary]==3.2.3 pgvector==0.2.0

# 5. Install pre-built FAISS-GPU (much faster and more reliable)
RUN pip install --no-cache-dir faiss-gpu && \
    echo "FAISS-GPU installation completed successfully"

# 6. Prepare model store directory
RUN mkdir -p /model-store

# 7. Copy models into the image
# Copy from the root models directory (go up from api/docker/)
COPY models/where.mar /model-store/ || true
COPY models/mapillary_WPCA128.pth.tar /model-store/ || true
RUN chown -R model-server:model-server /model-store || true

# List what models we have
RUN echo "Models in /model-store:" && ls -la /model-store/

# 8. Create TorchServe config for auto-loading
RUN echo 'inference_address=http://0.0.0.0:8080\n\
management_address=http://0.0.0.0:8081\n\
metrics_address=http://0.0.0.0:8082\n\
number_of_netty_threads=4\n\
job_queue_size=100\n\
model_store=/model-store\n\
load_models=where.mar\n\
default_workers_per_model=1' > /home/model-server/config.properties

# Ensure the config file has proper permissions
RUN chown model-server:model-server /home/model-server/config.properties

# 9. Create improved startup script with auto-loading support
RUN echo '#!/bin/bash\n\
set -e\n\
\n\
echo "Starting WhereIsThisPlace services..."\n\
echo "Models available in /model-store:"\n\
ls -la /model-store/\n\
\n\
# Start TorchServe with the config file that auto-loads models\n\
echo "Starting TorchServe with auto-loading configuration..."\n\
torchserve \\\n\
    --start \\\n\
    --model-store /model-store \\\n\
    --ts-config /home/model-server/config.properties \\\n\
    --ncs\n\
\n\
# Wait for TorchServe to be ready and models to load\n\
echo "Waiting for TorchServe to start and load models..."\n\
for i in {1..60}; do\n\
    if curl -s http://localhost:8081/ping > /dev/null 2>&1; then\n\
        echo "TorchServe is running!"\n\
        # Check if model is loaded\n\
        if curl -s http://localhost:8081/models/where 2>/dev/null | grep -q "where"; then\n\
            echo "✅ Model '"'"'where'"'"' is loaded and ready!"\n\
            break\n\
        fi\n\
    fi\n\
    if [ $i -eq 60 ]; then\n\
        echo "⚠ Timeout waiting for model to load, continuing anyway..."\n\
        curl -s http://localhost:8081/models || echo "Failed to list models"\n\
    fi\n\
    sleep 2\n\
done\n\
\n\
# Show loaded models\n\
echo "Currently loaded models:"\n\
curl -s http://localhost:8081/models || echo "Failed to list models"\n\
\n\
echo "Starting FastAPI application..."\n\
cd /app\n\
# Based on the repository structure, main.py is at api/api/main.py\n\
# and should be run as api.main:app\n\
exec uvicorn api.main:app --host 0.0.0.0 --port 8000\n\
' > /start.sh && chmod +x /start.sh

# 10. Expose ports
# 8080: TorchServe inference API
# 8081: TorchServe management API (internal)
# 8000: FastAPI application
EXPOSE 8080 8081 8000

# 11. Set default command
CMD ["/start.sh"]